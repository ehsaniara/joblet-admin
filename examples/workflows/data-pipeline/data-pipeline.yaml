version: "3.0"

# Data processing pipeline with file uploads and shared volume
jobs:
  extract-data:
    command: "python3"
    args: [ "extract.py" ]
    runtime: "python-3.11-ml"
    uploads:
      files: [ "extract.py" ]
    volumes: [ "data-pipeline" ]
    resources:
      max_memory: 1024

  validate-data:
    command: "python3"
    args: [ "validate.py" ]
    runtime: "python-3.11-ml"
    uploads:
      files: [ "validate.py" ]
    volumes: [ "data-pipeline" ]
    requires:
      - extract-data: "COMPLETED"

  transform-data:
    command: "python3"
    args: [ "transform.py" ]
    runtime: "python-3.11-ml"
    uploads:
      files: [ "transform.py" ]
    volumes: [ "data-pipeline" ]
    requires:
      - validate-data: "COMPLETED"
    resources:
      max_cpu: 50
      max_memory: 2048
  
  load-to-warehouse:
    command: "python3"
    args: [ "load.py" ]
    runtime: "python-3.11-ml"
    uploads:
      files: [ "load.py" ]
    volumes: [ "data-pipeline" ]
    requires:
      - transform-data: "COMPLETED"

  generate-report:
    command: "python3"
    args: [ "report.py" ]
    runtime: "python-3.11-ml"
    uploads:
      files: [ "report.py" ]
    volumes: [ "data-pipeline" ]
    requires:
      - load-to-warehouse: "COMPLETED"

  cleanup:
    command: "rm"
    args: [ "-rf", "data/", "*.pyc" ]
    volumes: [ "data-pipeline" ]
    requires:
      - generate-report: "COMPLETED"