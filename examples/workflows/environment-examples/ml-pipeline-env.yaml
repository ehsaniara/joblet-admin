version: "3.0"

# Machine Learning Pipeline with Environment Variables
# Shows how to pass model parameters and configuration between ML training jobs

jobs:
  # Setup ML experiment configuration
  setup-experiment:
    command: "python3"
    args: [ "-c", "import os, json; config = {'experiment_id': os.environ['EXPERIMENT_ID'], 'model_type': os.environ['MODEL_TYPE'], 'dataset': os.environ['DATASET_NAME'], 'target_metric': os.environ['TARGET_METRIC'], 'random_seed': int(os.environ['RANDOM_SEED'])}; json.dump(config, open('/volumes/ml-data/experiment_config.json', 'w'), indent=2); print(f'Experiment {config[\"experiment_id\"]} initialized'); print(f'Model: {config[\"model_type\"]}'); print(f'Dataset: {config[\"dataset\"]}')" ]
    environment:
      EXPERIMENT_ID: "exp-customer-churn-v3"
      MODEL_TYPE: "gradient_boosting"
      DATASET_NAME: "customer_behavior_2024"
      TARGET_METRIC: "auc_roc"
      RANDOM_SEED: "42"
      ML_FRAMEWORK: "scikit-learn"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    resources:
      max_memory: 512

  # Data preprocessing
  preprocess-data:
    command: "python3"
    args: [ "-c", "import os, json; config = json.load(open('/volumes/ml-data/experiment_config.json')); print(f'Preprocessing {config[\"dataset\"]}'); preprocessing = {'scaling': os.environ['SCALING_METHOD'], 'missing_strategy': os.environ['MISSING_VALUE_STRATEGY'], 'feature_selection': os.environ['FEATURE_SELECTION'], 'train_split': float(os.environ['TRAIN_SPLIT']), 'val_split': float(os.environ['VAL_SPLIT']), 'test_split': float(os.environ['TEST_SPLIT'])}; preprocessing.update(config); json.dump(preprocessing, open('/volumes/ml-data/preprocessing_config.json', 'w'), indent=2); print(f'Preprocessing configured: {preprocessing[\"scaling\"]} scaling, {preprocessing[\"missing_strategy\"]} for missing values')" ]
    environment:
      SCALING_METHOD: "standard"
      MISSING_VALUE_STRATEGY: "median"
      FEATURE_SELECTION: "mutual_info"
      TRAIN_SPLIT: "0.7"
      VAL_SPLIT: "0.15"
      TEST_SPLIT: "0.15"
      MAX_FEATURES: "50"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    requires:
      - setup-experiment: "COMPLETED"
    resources:
      max_memory: 2048
      max_cpu: 50

  # Feature engineering
  feature-engineering:
    command: "python3"
    args: [ "-c", "import os, json; config = json.load(open('/volumes/ml-data/preprocessing_config.json')); print(f'Feature engineering for {config[\"experiment_id\"]}'); features = {'polynomial_degree': int(os.environ['POLYNOMIAL_DEGREE']), 'interaction_features': os.environ['INTERACTION_FEATURES'] == 'true', 'time_features': os.environ['TIME_FEATURES'] == 'true', 'categorical_encoding': os.environ['CATEGORICAL_ENCODING'], 'text_vectorization': os.environ['TEXT_VECTORIZATION']}; features.update(config); json.dump(features, open('/volumes/ml-data/feature_config.json', 'w'), indent=2); print(f'Feature engineering: {features[\"categorical_encoding\"]} encoding, polynomial degree {features[\"polynomial_degree\"]}')" ]
    environment:
      POLYNOMIAL_DEGREE: "2"
      INTERACTION_FEATURES: "true"
      TIME_FEATURES: "true"
      CATEGORICAL_ENCODING: "target_encoding"
      TEXT_VECTORIZATION: "tfidf"
      N_GRAM_RANGE: "1,2"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    requires:
      - preprocess-data: "COMPLETED"
    resources:
      max_memory: 4096
      max_cpu: 75

  # Hyperparameter optimization
  hyperparameter-tuning:
    command: "python3"
    args: [ "-c", "import os, json; config = json.load(open('/volumes/ml-data/feature_config.json')); print(f'Hyperparameter tuning for {config[\"model_type\"]}'); tuning = {'optimization_method': os.environ['OPTIMIZATION_METHOD'], 'n_trials': int(os.environ['N_TRIALS']), 'cv_folds': int(os.environ['CV_FOLDS']), 'scoring': config['target_metric'], 'n_jobs': int(os.environ['N_JOBS']), 'timeout_minutes': int(os.environ['TIMEOUT_MINUTES'])}; tuning.update(config); best_params = {'max_depth': 8, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.8}; tuning['best_params'] = best_params; tuning['best_score'] = 0.847; json.dump(tuning, open('/volumes/ml-data/tuning_results.json', 'w'), indent=2); print(f'Best {config[\"target_metric\"]}: {tuning[\"best_score\"]:.3f}')" ]
    environment:
      OPTIMIZATION_METHOD: "bayesian"
      N_TRIALS: "100"
      CV_FOLDS: "5"
      N_JOBS: "4"
      TIMEOUT_MINUTES: "60"
      EARLY_STOPPING: "true"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    requires:
      - feature-engineering: "COMPLETED"
    resources:
      max_memory: 8192
      max_cpu: 90

  # Model training
  train-model:
    command: "python3"
    args: [ "-c", "import os, json; config = json.load(open('/volumes/ml-data/tuning_results.json')); print(f'Training {config[\"model_type\"]} model'); training = {'batch_size': int(os.environ['BATCH_SIZE']), 'max_epochs': int(os.environ['MAX_EPOCHS']), 'early_stopping_patience': int(os.environ['EARLY_STOPPING_PATIENCE']), 'validation_metric': config['target_metric'], 'save_checkpoints': os.environ['SAVE_CHECKPOINTS'] == 'true'}; training.update(config); final_metrics = {'train_auc': 0.923, 'val_auc': 0.847, 'test_auc': 0.841, 'precision': 0.82, 'recall': 0.75, 'f1_score': 0.78}; training['final_metrics'] = final_metrics; training['model_path'] = f'/volumes/ml-models/{config[\"experiment_id\"]}_model.pkl'; json.dump(training, open('/volumes/ml-data/training_results.json', 'w'), indent=2); print(f'Model trained: Test AUC = {final_metrics[\"test_auc\"]:.3f}')" ]
    environment:
      BATCH_SIZE: "1024"
      MAX_EPOCHS: "100"
      EARLY_STOPPING_PATIENCE: "10"
      SAVE_CHECKPOINTS: "true"
      CHECKPOINT_FREQUENCY: "5"
      MODEL_FORMAT: "pickle"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    requires:
      - hyperparameter-tuning: "COMPLETED"
    resources:
      max_memory: 8192
      max_cpu: 80

  # Model evaluation
  evaluate-model:
    command: "python3"
    args: [ "-c", "import os, json; config = json.load(open('/volumes/ml-data/training_results.json')); print(f'Evaluating model {config[\"experiment_id\"]}'); evaluation = {'evaluation_dataset': os.environ['EVALUATION_DATASET'], 'metrics_to_compute': os.environ['METRICS_TO_COMPUTE'].split(','), 'generate_plots': os.environ['GENERATE_PLOTS'] == 'true', 'feature_importance': os.environ['FEATURE_IMPORTANCE'] == 'true'}; evaluation.update(config); detailed_metrics = {'confusion_matrix': [[850, 123], [89, 678]], 'classification_report': 'precision/recall computed', 'roc_curve_auc': 0.841, 'pr_curve_auc': 0.789, 'feature_importance_top10': ['feature_1', 'feature_2', 'feature_3']}; evaluation['detailed_metrics'] = detailed_metrics; evaluation['model_performance_grade'] = 'A-' if detailed_metrics['roc_curve_auc'] > 0.8 else 'B'; json.dump(evaluation, open('/volumes/ml-data/evaluation_results.json', 'w'), indent=2); print(f'Model performance grade: {evaluation[\"model_performance_grade\"]}')" ]
    environment:
      EVALUATION_DATASET: "holdout_test_set"
      METRICS_TO_COMPUTE: "auc,precision,recall,f1,accuracy"
      GENERATE_PLOTS: "true"
      FEATURE_IMPORTANCE: "true"
      BIAS_ANALYSIS: "true"
      FAIRNESS_METRICS: "demographic_parity,equalized_odds"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    requires:
      - train-model: "COMPLETED"
    resources:
      max_memory: 4096
      max_cpu: 60

  # Model validation
  validate-model:
    command: "python3"
    args: [ "-c", "import os, json; config = json.load(open('/volumes/ml-data/evaluation_results.json')); print(f'Validating model for deployment'); validation = {'min_auc_threshold': float(os.environ['MIN_AUC_THRESHOLD']), 'max_bias_threshold': float(os.environ['MAX_BIAS_THRESHOLD']), 'performance_degradation_threshold': float(os.environ['PERFORMANCE_DEGRADATION_THRESHOLD']), 'business_impact_threshold': float(os.environ['BUSINESS_IMPACT_THRESHOLD'])}; validation.update(config); current_auc = config['detailed_metrics']['roc_curve_auc']; validation['deployment_ready'] = current_auc >= validation['min_auc_threshold']; validation['validation_status'] = 'PASSED' if validation['deployment_ready'] else 'FAILED'; validation['deployment_recommendation'] = 'DEPLOY' if validation['deployment_ready'] else 'DO_NOT_DEPLOY'; json.dump(validation, open('/volumes/ml-data/validation_results.json', 'w'), indent=2); print(f'Validation status: {validation[\"validation_status\"]}'); print(f'Recommendation: {validation[\"deployment_recommendation\"]}')" ]
    environment:
      MIN_AUC_THRESHOLD: "0.8"
      MAX_BIAS_THRESHOLD: "0.1"
      PERFORMANCE_DEGRADATION_THRESHOLD: "0.05"
      BUSINESS_IMPACT_THRESHOLD: "0.15"
      A_B_TEST_REQUIRED: "true"
      SHADOW_MODE_DURATION: "7"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    requires:
      - evaluate-model: "COMPLETED"
    resources:
      max_memory: 2048

  # Deploy model (conditional)
  deploy-model:
    command: "python3"
    args: [ "-c", "import os, json; config = json.load(open('/volumes/ml-data/validation_results.json')); print(f'Deploying model {config[\"experiment_id\"]}'); if config['deployment_ready']: deployment = {'deployment_environment': os.environ['DEPLOYMENT_ENVIRONMENT'], 'serving_framework': os.environ['SERVING_FRAMEWORK'], 'api_endpoint': os.environ['API_ENDPOINT'], 'monitoring_enabled': os.environ['MONITORING_ENABLED'] == 'true', 'auto_scaling': os.environ['AUTO_SCALING'] == 'true'}; deployment.update(config); deployment['deployment_timestamp'] = '2024-08-18T10:30:00Z'; deployment['model_version'] = f'v{os.environ[\"MODEL_VERSION\"]}'; deployment['deployment_status'] = 'SUCCESS'; json.dump(deployment, open('/volumes/ml-models/deployment_info.json', 'w'), indent=2); print(f'Model deployed successfully to {deployment[\"deployment_environment\"]}'); print(f'API endpoint: {deployment[\"api_endpoint\"]}'); else: print('Model deployment skipped - validation failed')" ]
    environment:
      DEPLOYMENT_ENVIRONMENT: "production"
      SERVING_FRAMEWORK: "mlflow"
      API_ENDPOINT: "https://api.company.com/ml/customer-churn/predict"
      MONITORING_ENABLED: "true"
      AUTO_SCALING: "true"
      MODEL_VERSION: "3.1.0"
      REPLICAS: "3"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    requires:
      - validate-model: "COMPLETED"
    resources:
      max_memory: 2048

  # Generate experiment report
  generate-ml-report:
    command: "python3"
    args: [ "-c", "import os, json; validation_config = json.load(open('/volumes/ml-data/validation_results.json')); print(f'Generating ML experiment report'); report = {'experiment_summary': {'experiment_id': validation_config['experiment_id'], 'model_type': validation_config['model_type'], 'final_auc': validation_config['detailed_metrics']['roc_curve_auc'], 'deployment_status': validation_config['validation_status'], 'recommendation': validation_config['deployment_recommendation']}, 'report_format': os.environ['REPORT_FORMAT'], 'include_artifacts': os.environ['INCLUDE_ARTIFACTS'] == 'true', 'notification_channels': os.environ['NOTIFICATION_CHANNELS'].split(',')}; report['experiment_artifacts'] = ['model.pkl', 'evaluation_plots.png', 'feature_importance.csv', 'training_logs.txt'] if report['include_artifacts'] else []; json.dump(report, open('/volumes/ml-data/experiment_report.json', 'w'), indent=2); print(f'Experiment report generated: {report[\"report_format\"]} format'); print(f'Final model AUC: {report[\"experiment_summary\"][\"final_auc\"]:.3f}')" ]
    environment:
      REPORT_FORMAT: "comprehensive"
      INCLUDE_ARTIFACTS: "true"
      NOTIFICATION_CHANNELS: "mlflow,slack,email"
      EXPERIMENT_TRACKING_URL: "https://mlflow.company.com"
      STAKEHOLDER_EMAILS: "data-science@company.com,product@company.com"
    volumes: [ "ml-data", "ml-models" ]
    runtime: "python-3.11-ml"
    requires:
      - deploy-model: "COMPLETED"
    resources:
      max_memory: 1024