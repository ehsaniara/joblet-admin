version: "3.0"

# Distributed ML Training Pipeline with Environment Variables
# Shows how to run distributed training with data preprocessing and model evaluation

jobs:
  # Data preprocessing job
  preprocess-data:
    command: "python3"
    args: [ "preprocess.py" ]
    environment:
      INPUT_PATH: "/volumes/raw-data"
      OUTPUT_PATH: "/volumes/processed-data"
      DATA_FORMAT: "parquet"
      NUM_SHARDS: "8"
      VALIDATION_SPLIT: "0.2"
      TEST_SPLIT: "0.1"
      RANDOM_SEED: "42"
    volumes: [ "raw-data", "processed-data" ]
    resources:
      max_memory: 4096
      max_cpu: 200

  # Initialize training environment
  setup-training:
    command: "python3"
    args: [ "setup_training.py" ]
    environment:
      MODEL_NAME: "transformer-large"
      CHECKPOINT_DIR: "/volumes/checkpoints"
      LOG_DIR: "/volumes/logs"
      CONFIG_PATH: "/volumes/config/training.json"
      DISTRIBUTED: "true"
      NUM_GPUS: "4"
    secret_environment:
      WANDB_API_KEY: "your_wandb_key_here"
      HF_TOKEN: "huggingface_token"
    volumes: [ "checkpoints", "logs", "config" ]
    requires:
      - preprocess-data: "COMPLETED"
    resources:
      max_memory: 2048

  # Parameter server for distributed training
  parameter-server:
    command: "python3"
    args: [ "parameter_server.py" ]
    environment:
      ROLE: "ps"
      PS_HOST: "0.0.0.0"
      PS_PORT: "2222"
      NUM_WORKERS: "4"
      OPTIMIZER: "adam"
      LEARNING_RATE: "0.0001"
      GRADIENT_CLIPPING: "1.0"
    secret_environment:
      CLUSTER_KEY: "secure_cluster_key"
    volumes: [ "checkpoints", "logs" ]
    requires:
      - setup-training: "COMPLETED"
    resources:
      max_memory: 8192
      max_cpu: 400

  # Worker node 0 (with GPU)
  worker-0:
    command: "python3"
    args: [ "train_worker.py" ]
    environment:
      ROLE: "worker"
      WORKER_ID: "0"
      PS_HOST: "parameter-server"
      PS_PORT: "2222"
      BATCH_SIZE: "32"
      ACCUMULATION_STEPS: "4"
      DEVICE: "cuda:0"
      MIXED_PRECISION: "true"
    secret_environment:
      CLUSTER_KEY: "secure_cluster_key"
    volumes: [ "processed-data", "checkpoints", "logs" ]
    requires:
      - parameter-server: "RUNNING"
    resources:
      max_memory: 16384
      max_cpu: 400

  # Worker node 1 (with GPU)
  worker-1:
    command: "python3"
    args: [ "train_worker.py" ]
    environment:
      ROLE: "worker"
      WORKER_ID: "1"
      PS_HOST: "parameter-server"
      PS_PORT: "2222"
      BATCH_SIZE: "32"
      ACCUMULATION_STEPS: "4"
      DEVICE: "cuda:1"
      MIXED_PRECISION: "true"
    secret_environment:
      CLUSTER_KEY: "secure_cluster_key"
    volumes: [ "processed-data", "checkpoints", "logs" ]
    requires:
      - parameter-server: "RUNNING"
    resources:
      max_memory: 16384
      max_cpu: 400

  # Worker node 2 (with GPU)
  worker-2:
    command: "python3"
    args: [ "train_worker.py" ]
    environment:
      ROLE: "worker"
      WORKER_ID: "2"
      PS_HOST: "parameter-server"
      PS_PORT: "2222"
      BATCH_SIZE: "32"
      ACCUMULATION_STEPS: "4"
      DEVICE: "cuda:2"
      MIXED_PRECISION: "true"
    secret_environment:
      CLUSTER_KEY: "secure_cluster_key"
    volumes: [ "processed-data", "checkpoints", "logs" ]
    requires:
      - parameter-server: "RUNNING"
    resources:
      max_memory: 16384
      max_cpu: 400

  # Worker node 3 (with GPU)
  worker-3:
    command: "python3"
    args: [ "train_worker.py" ]
    environment:
      ROLE: "worker"
      WORKER_ID: "3"
      PS_HOST: "parameter-server"
      PS_PORT: "2222"
      BATCH_SIZE: "32"
      ACCUMULATION_STEPS: "4"
      DEVICE: "cuda:3"
      MIXED_PRECISION: "true"
    secret_environment:
      CLUSTER_KEY: "secure_cluster_key"
    volumes: [ "processed-data", "checkpoints", "logs" ]
    requires:
      - parameter-server: "RUNNING"
    resources:
      max_memory: 16384
      max_cpu: 400

  # Model evaluation
  evaluate-model:
    command: "python3"
    args: [ "evaluate.py" ]
    environment:
      MODEL_PATH: "/volumes/checkpoints/best_model"
      TEST_DATA_PATH: "/volumes/processed-data/test"
      METRICS_OUTPUT: "/volumes/logs/metrics.json"
      BATCH_SIZE: "64"
      DEVICE: "cuda"
    volumes: [ "processed-data", "checkpoints", "logs" ]
    requires:
      - worker-0: "COMPLETED"
      - worker-1: "COMPLETED"
      - worker-2: "COMPLETED"
      - worker-3: "COMPLETED"
    resources:
      max_memory: 8192
      max_cpu: 200

  # Generate training report
  generate-report:
    command: "python3"
    args: [ "generate_report.py" ]
    environment:
      LOGS_PATH: "/volumes/logs"
      METRICS_PATH: "/volumes/logs/metrics.json"
      OUTPUT_PATH: "/volumes/logs/training_report.html"
      INCLUDE_VISUALIZATIONS: "true"
    volumes: [ "logs" ]
    requires:
      - evaluate-model: "COMPLETED"
    resources:
      max_memory: 2048