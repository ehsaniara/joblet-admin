jobs:
  # ETL Job with multiple steps
  etl:
    command: "bash"
    args: [ "-c", "set -e\necho 'Starting ETL Pipeline...'\necho 'Extracting data from source...'\npython3 /scripts/extract.py --source=/volumes/raw-data --output=/tmp/extracted\necho 'Transforming data...'\npython3 /scripts/transform.py --input=/tmp/extracted --output=/tmp/transformed\necho 'Loading data to destination...'\npython3 /scripts/load.py --input=/tmp/transformed --destination=/volumes/processed-data\necho 'ETL Pipeline completed!'" ]
    runtime: "python-3.11-ml"
    uploads:
      files: [ "scripts/" ]
    volumes: [ "raw-data", "processed-data", "etl-logs" ]
    resources:
      max_memory: 2048
      max_cpu: 75
      max_io_bps: 5242880

  # Data validation job
  validate:
    command: "python3"
    args: [ "validate.py", "--input=/volumes/processed-data", "--report=/volumes/validation-reports" ]
    runtime: "python-3.11-ml"
    uploads:
      files: [ "validate.py", "validation_rules.yaml" ]
    volumes: [ "processed-data", "validation-reports" ]
    resources:
      max_memory: 512
    requires:
      - etl: "COMPLETED"

  # Data compression job
  compress:
    command: "bash"
    args: [ "-c", "find /volumes/processed-data -type f -mtime +7 -exec gzip {} \\;\necho 'Compression completed'" ]
    runtime: "python-3.11-ml"
    volumes: [ "processed-data" ]
    resources:
      max_memory: 256
      max_cpu: 25
      max_io_bps: 2097152
    requires:
      - validate: "COMPLETED"

  # Database backup job
  db-backup:
    command: "bash"
    args: [ "-c", "DATE=$(date +%Y%m%d_%H%M%S)\npg_dump -h db-host -U dbuser -d mydb | gzip > /volumes/backups/db_backup_${DATE}.sql.gz\nfind /volumes/backups -name 'db_backup_*.sql.gz' -mtime +7 -delete" ]
    runtime: "python-3.11-ml"
    volumes: [ "backups" ]
    resources:
      max_memory: 512
      max_io_bps: 3145728

  # Report generator
  report:
    command: "python3"
    args: [ "generate_report.py", "--data=/volumes/processed-data", "--output=/volumes/reports", "--format=pdf" ]
    runtime: "python-3.11-ml"
    uploads:
      files: [ "generate_report.py", "report_template.html" ]
    volumes: [ "processed-data", "reports" ]
    resources:
      max_memory: 1024
      max_cpu: 60
    requires:
      - compress: "COMPLETED"

  # Data sync job
  sync:
    command: "rsync"
    args: [ "-avz", "--delete", "/volumes/source-data/", "/volumes/backup-data/" ]
    runtime: "python-3.11-ml"
    volumes: [ "source-data", "backup-data" ]
    resources:
      max_memory: 256
      max_cpu: 30
      max_io_bps: 4194304

  # Cleanup job
  cleanup:
    command: "bash"
    args: [ "-c", "find /tmp -type f -mtime +3 -delete 2>/dev/null || true\nfind /volumes/logs -name '*.log' -size +100M -exec gzip {} \\;\nfind /volumes/logs -name '*.log.gz' -mtime +30 -delete\necho 'Cleanup completed'" ]
    runtime: "python-3.11-ml"
    volumes: [ "logs" ]
    resources:
      max_memory: 128
      max_cpu: 20
    requires:
      - report: "COMPLETED"